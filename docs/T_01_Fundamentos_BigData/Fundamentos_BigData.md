
<div class="titulo">
   Fundamentos y Ecosistema Moderno de Big Data
</div>
#  

¬°Bienvenidos al curso de Big Data Aplicado! Este primer m√≥dulo es la puerta de entrada al fascinante universo de los datos a gran escala. Olv√≠date de las definiciones abstractas; aqu√≠ vamos a entender **por qu√©** el Big Data es una de las √°reas m√°s revolucionarias de la tecnolog√≠a actual.

No nos quedaremos solo en la teor√≠a de las "5 V"; exploraremos el **ecosistema de herramientas real** que usan empresas como Netflix, Airbnb o Spotify, conocido como el "Modern Data Stack". Entenderemos los distintos roles profesionales que existen en este campo y, lo m√°s importante, presentaremos los **dos proyectos que ser√°n nuestra gu√≠a pr√°ctica** durante las pr√≥ximas sesiones de clase. El objetivo es que, al finalizar este m√≥dulo, tengas un mapa completo del territorio que vamos a explorar y los emocionantes retos que vamos a superar juntos.

## 1. ¬øQu√© es Big Data? M√°s all√° de "muchos datos"

El Big Data se refiere a conjuntos de datos masivos y complejos que, por su volumen, variedad y velocidad, superan la capacidad de las herramientas tradicionales para su almacenamiento y procesamiento. Su principal objetivo es extraer informaci√≥n valiosa, patrones y conocimientos para tomar mejores decisiones, optimizar procesos y desarrollar soluciones inteligentes en diversos √°mbitos como la medicina, el marketing o la gesti√≥n del tr√°fico. 

Imagina que pasas de tener una libreta de contactos (un Excel) a tener que gestionar la agenda de tel√©fonos de todo un pa√≠s, que adem√°s se actualiza cada segundo. Esa es la esencia del Big Data: los problemas y oportunidades que surgen cuando los datos superan la capacidad de las herramientas tradicionales.

Para entenderlo mejor, usamos el modelo de las **5 "V"**:

### 1.1. Las 5 "V" del Big Data

- **Volumen (Cantidad):** Se refiere a la escala masiva de los datos generados. Ya no hablamos de Megabytes o Gigabytes, sino de **Terabytes, Petabytes o incluso Exabytes**.
    - *Ejemplo:* Las horas de v√≠deo subidas a YouTube cada minuto, o los datos de transacciones de una cadena de supermercados global.
    - *Reto:* ¬øC√≥mo almacenamos y accedemos a esta cantidad ingente de informaci√≥n de forma eficiente y econ√≥mica?
- **Velocidad (Ritmo):** Se refiere al ritmo al que los datos se crean, transmiten y procesan. Muchas veces, necesitamos analizarlos en **tiempo real** o casi real.
    - *Ejemplo:* El torrente de tuits durante la final de un mundial, los datos de sensores de un coche aut√≥nomo o las transacciones con tarjeta de cr√©dito que deben ser validadas en milisegundos.
    - *Reto:* ¬øC√≥mo capturamos y procesamos datos que "vuelan" sin perder informaci√≥n valiosa?
- **Variedad (Formatos):** Los datos ya no son solo tablas ordenadas en una base de datos. Vienen en m√∫ltiples formatos.
    - **Datos Estructurados:** Altamente organizados, como una tabla de Excel o una base de datos SQL.
    - **Datos Semi-estructurados:** No encajan en una tabla, pero tienen una estructura interna, como un fichero JSON o XML.
    - **Datos No Estructurados:** No tienen un modelo predefinido, como un email, una imagen, un v√≠deo, un audio o un post en redes sociales.
    - *Ejemplo:* Un √∫nico post de Instagram puede contener una imagen (no estructurado), un texto con #hashtags (semi-estructurado) y metadatos como la geolocalizaci√≥n (estructurado).
    - *Reto:* ¬øC√≥mo extraemos informaci√≥n √∫til de esta mezcla heterog√©nea de formatos?

??? info  "Tipos de datos"
    | Tipo de Datos | Descripcion | Formatos comunes | Ejemplos | Observaciones| 
    | -- | -- |-- | -- | -- |
    |**Archivos de Datos Estructurados** | Datos organizados en un formato con una estructura fija y predefinida, como las tablas de una base de datos relacional.| CSV | Archivos de hojas de c√°lculo, bases de datos SQL y archivos CSV. |Los archivos estructurados son eficientes para almacenar datos tabulares, pero pueden ser menos flexibles para datos variados o no estructurados. |
    | **Archivos de Datos Semiestructurados** | No siguen una estructura fija como los datos estructurados, pero tienen cierta organizaci√≥n o jerarqu√≠a, como en el caso de archivos JSON o XML. | JSON y XML |  Archivos JSON que almacenan configuraciones, datos de sensores o informaci√≥n de productos. | Los datos semiestructurados son ideales para representar informaci√≥n jer√°rquica, como configuraciones de aplicaciones o datos de sensores. |
    | **Archivos de Datos No Estructurados** | Carecen de una organizaci√≥n o formato definido y pueden incluir archivos de im√°genes, videos, documentos de texto sin formato y audio, entre otros. | | Im√°genes JPEG, videos MP4, documentos de texto sin formato, grabaciones de audio, etc. | Se utilizan herramientas de procesamiento de medios, visi√≥n por computadora, reconocimiento de voz y an√°lisis de texto para extraer informaci√≥n de datos no estructurados. |
    | **Archivos de Log** | Registran eventos y actividades, como transacciones, errores o interacciones de usuarios. |  |  | Herramientas como Splunk, ELK Stack (Elasticsearch, Logstash, Kibana) y Apache Flume se utilizan para analizar y visualizar datos de registros. |

- **Veracidad (Confianza):** Se refiere a la calidad y fiabilidad de los datos. No todos los datos son precisos. Pueden tener ruido, sesgos, inconsistencias o simplemente ser falsos.
    - *Ejemplo:* Rese√±as falsas de productos en Amazon, errores en lecturas de sensores por un fallo de hardware, o datos introducidos manualmente con erratas.
    - *Reto:* ¬øC√≥mo limpiamos los datos y nos aseguramos de que podemos confiar en ellos para tomar decisiones? *Garbage In, Garbage Out* (si entra basura, sale basura).
- **Valor (Prop√≥sito):** Esta es la "V" m√°s importante. De nada sirve tener un oc√©ano de datos si no podemos extraer de √©l conocimiento √∫til (**insights**) que nos permita tomar mejores decisiones, optimizar procesos, crear nuevos productos o entender mejor a nuestros clientes.
    - *Ejemplo:* Netflix analiza qu√© ven sus millones de usuarios (volumen, velocidad, variedad) para decidir qu√© series producir (valor).
    - *Reto y Objetivo Final:* ¬øC√≥mo transformamos los datos crudos en valor tangible para una organizaci√≥n?

### 1.2. Roles Profesionales en el Mundo del Dato

Para gestionar estas 5 "V", han surgido roles especializados. Pensemos en ellos como un equipo de construcci√≥n:

- **üë∑‚Äç‚ôÄÔ∏è Ingeniero de Datos (Data Engineer):** Es el arquitecto e ingeniero civil del mundo de los datos. Su trabajo es dise√±ar, construir y mantener las "autopistas" por las que viajar√°n los datos. Crea los pipelines (tuber√≠as) que recogen datos de las fuentes (APIs, bases de datos, ficheros) y los llevan a un lugar centralizado (Data Lake o Data Warehouse) de forma limpia y eficiente. **En este curso, adoptaremos principalmente este rol.**
    - *Herramientas:* SQL, Python, **Apache NiFi**, Airflow, Spark, bases de datos NoSQL.
- **üë®‚Äçüíº Analista de Datos (Data Analyst):** Es el int√©rprete o el traductor. Una vez que los datos est√°n en el destino, el analista los explora, los analiza y crea informes y visualizaciones (dashboards) para responder a preguntas de negocio concretas. Busca tendencias, patrones y "qu√© ha pasado".
    - *Herramientas:* SQL, **Power BI**, Tableau, Excel.
- **üë©‚Äçüî¨ Cient√≠fico de Datos (Data Scientist):** Es el "adivino" o estratega. Va un paso m√°s all√° del analista. Utiliza m√©todos estad√≠sticos y algoritmos de Machine Learning para hacer predicciones y responder a "qu√© pasar√°" o "qu√© podemos hacer para que algo ocurra".
    - *Herramientas:* Python (con librer√≠as como Scikit-learn, TensorFlow), R, estad√≠stica, Machine Learning.


## 2. El Ecosistema de Datos Moderno

El "s√≥tano de servidores" de las empresas ha dado paso a la nube. El ecosistema moderno de datos se basa en servicios flexibles, escalables y potentes ofrecidos por proveedores cloud.

**¬øPor qu√© la nube?**

- **Escalabilidad:** Paga solo por lo que usas y aumenta la capacidad con un clic.
- **Flexibilidad:** Accede a una enorme gama de herramientas especializadas.
- **Mantenimiento Reducido:** El proveedor se encarga de la infraestructura f√≠sica.

**Principales Proveedores:**

- **AWS (Amazon Web Services):** El pionero y l√≠der del mercado.
- **Microsoft Azure:** Fuerte integraci√≥n con el ecosistema de Microsoft.
- **GCP (Google Cloud Platform):** Muy potente en an√°lisis de datos y Machine Learning.

     

### 2.1.El Ciclo de Vida del Dato en el Ecosistema Moderno

Pensemos en el viaje que hacen los datos como una cadena de montaje. Cada etapa tiene un prop√≥sito espec√≠fico:

1. **Ingesta (Ingestion):** Es el punto de partida. ¬øC√≥mo metemos los datos en nuestro sistema?
    - *Fuentes:* APIs de redes sociales, sensores IoT (como en nuestro **Proyecto Smart City**), bases de datos de la empresa, ficheros CSV (como en nuestro **Proyecto An√°lisi Acad√©mico**).
    - *Herramientas:* **Apache NiFi**, Fivetran, Airbyte.
2. **Almacenamiento (Storage):** ¬øD√≥nde guardamos los datos?
    - **Data Lake:** Un gran "lago" de datos crudos. Almacena todo en su formato original. Es barato y muy flexible. Ideal para datos no estructurados y para que los Cient√≠ficos de Datos exploren.
    - **Data Warehouse:** Un "almac√©n" de datos estructurados, limpios y ordenados. La informaci√≥n est√° modelada y lista para ser analizada y generar informes (BI).
    - **Data Lakehouse:** La arquitectura moderna que une ambos mundos. Permite tener la flexibilidad y bajo coste de un Data Lake con las capacidades de gesti√≥n y rendimiento de un Data Warehouse.
3. **Procesamiento (Processing - ETL vs. ELT):** ¬øC√≥mo transformamos los datos crudos en datos √∫tiles?
    - **ETL (Extract, Transform, Load):** El m√©todo tradicional. Se extraen los datos, se transforman en un servidor intermedio y luego se cargan ya limpios en el Data Warehouse.
    - **ELT (Extract, Load, Transform):** El m√©todo moderno, potenciado por la nube. Se extraen los datos y se cargan **inmediatamente** en el Data Lake/Warehouse. La transformaci√≥n se hace *despu√©s*, aprovechando la potencia de c√°lculo del sistema de almacenamiento. Es m√°s r√°pido y flexible.
    - *Herramientas:* **Apache NiFi** (ideal para ETL visual), dbt (data build tool), Apache Spark.
4. **Orquestaci√≥n (Orchestration): El Director de Orquesta**
    - **¬øQu√© es?** Es el proceso de **automatizar, programar y coordinar** la ejecuci√≥n de todos nuestros flujos de datos. Un flujo de ETL no sirve de mucho si tenemos que ejecutarlo a mano cada d√≠a. La orquestaci√≥n se encarga de que todo funcione solo.
    - **Analog√≠a:** Piensa en los flujos de NiFi como los m√∫sicos de una orquesta. Cada uno sabe tocar su instrumento (procesar datos). El orquestador es el **director de orquesta**: les dice cu√°ndo empezar, en qu√© orden tocar, y qu√© hacer si uno de ellos falla.
    - **Prop√≥sito:** Define las dependencias ("*no empieces a procesar los datos hasta que no se hayan cargado por completo*"), gestiona los reintentos en caso de error y nos da una visi√≥n centralizada de la salud de todos nuestros procesos.
    - *Herramientas:* **Apache Airflow** (el est√°ndar de la industria, que define los flujos como c√≥digo en "DAGs" o Grafos Ac√≠clicos Dirigidos), Mage, Prefect.
5. **Visualizaci√≥n y An√°lisis (Analytics & BI):** ¬øC√≥mo le damos sentido a los datos?
    - Es la capa final, donde los datos se convierten en gr√°ficos, informes y cuadros de mando interactivos que aportan **valor** al negocio.
    - *Herramientas:* **Microsoft Power BI** y Tableau (para Business Intelligence), **Grafana** (para monitorizaci√≥n y datos en tiempo real).

/// html | div[style='text-align: center;']

```mermaid
graph LR;
    A(1. Ingesta <br> Apache NiFi) --> B(2. Almacenamiento <br> Data Lake, DW);
    B --> C(3. Procesamiento <br> ETL / ELT <br> NiFi, Spark);
    C --> D(4. Orquestaci√≥n <br> Apache Airflow);
    D --> E(5. Visualizaci√≥n y An√°lisis <br> Power BI, Grafana);
```
///

## 3. Presentaci√≥n de los Proyectos del Curso

La mejor forma de aprender es construyendo. Estos dos proyectos ser√°n nuestro hilo conductor. Cada m√≥dulo nos dar√° una pieza para avanzar en ellos.

### 3.1. Proyecto 1: Smart City - El Pulso de la Ciudad en Tiempo Real

- **El Reto:** Simular una ciudad inteligente. Capturaremos datos de sensores de temperatura, CO2 y calidad del agua, los procesaremos en tiempo real y crearemos un panel de control para monitorizar la "salud" de la ciudad.
- **Conceptos Clave que Aplicaremos:**
    - Ingesta de datos IoT (FIWARE).
    - Bases de datos NoSQL.
    - Creaci√≥n de flujos de datos en tiempo real con **Apache NiFi**.
    - Visualizaci√≥n de series temporales y alertas con **Grafana**.
- **Resultado Final:** Un dashboard operativo que nos muestre en directo qu√© est√° pasando en nuestra ciudad.

### 3.2. Proyecto 2: An√°lisis Acad√©mico - Inteligencia para la Educaci√≥n

- **El Reto:** Ayudar a un centro educativo a entender sus resultados acad√©micos. Partiremos de varios ficheros CSV "sucios" y los transformaremos en un sistema de Business Intelligence (BI) coherente que permita tomar decisiones.
- **Conceptos Clave que Aplicaremos:**
    - ETL por lotes para limpiar y unir datos de ficheros.
    - Dise√±o de un Data Warehouse con un **esquema en estrella**.
    - Uso de **Apache NiFi** para automatizar la limpieza y carga de datos.
    - Creaci√≥n de un cuadro de mando interactivo con **Power BI**.
- **Resultado Final:** Un informe en Power BI que permita al equipo directivo filtrar por curso, a√±o o ciclo y analizar tasas de √©xito, promociones, etc.

[Proyectos](../PDF/Proyectos.pdf){target="_blank"}

## 4. Pr√°ctica Inicial

Vamos a empezar a "pensar en datos" desde el primer d√≠a.

### Tarea 1: Dise√±o del ADN de los Sensores (Proyecto Smart City)

**Objetivo:** Entender la importancia de modelar los datos antes de empezar. Un buen dise√±o inicial nos ahorrar√° muchos problemas.

**Instrucciones:**

1. Crea un Repositorio Git Hub para ir a√±adido todo el trabajo del proyecto

2. Abre un editor de texto simple (Bloc de Notas, VS Code) o una hoja de c√°lculo.

3. Crea una entidad para el sensor de temperatura definiendo su estructura.

4. Crea una estructura similar para:

    - El sensor de CO2 (con un atributo `co2` en `ppm`)
    - El sensor de calidad del agua (con `ph`, `chlorine` en `mgL`, etc.). 

5. No te preocupes por la sintaxis perfecta, c√©ntrate en definir los **atributos, sus tipos de dato y sus unidades**.

### Tarea 2: La Primera Inspecci√≥n: (Proyecto An√°lisis Acad√©mico)

**Objetivo:** Desarrollar un "ojo cr√≠tico" para la calidad de los datos. Es una de las habilidades m√°s importantes de un profesional del dato.

  
**Instrucciones:**

1. Crea un Repositorio Git Hub para ir a√±adido todo el trabajo del proyecto
2. Descarga los ficheros CSV del proyecto acad√©mico.
3. √Åbrelos con Microsoft Excel, Google Sheets o un visor de CSV. **No modifiques nada todav√≠a.**
4. Convi√©rtete en un detective y responde a estas preguntas para cada fichero:
    - ¬øCu√°l es el separador de columnas (coma `,` o punto y coma `;`)?
    - ¬øLa primera fila contiene los nombres de las columnas (encabezados)? ¬øSon claros?
    - Inspecciona visualmente las primeras 20-30 filas. ¬øVes valores que te parezcan extra√±os o que faltan (celdas vac√≠as, "N/A", "s/d")?
    - ¬øLos formatos son consistentes? Por ejemplo, ¬ølas fechas est√°n siempre como `DD/MM/AAAA` o a veces cambian?
    - Identifica las "claves" o "IDs" que podr√≠an servir para relacionar unos ficheros con otros (ej: `id_alumno` en el fichero de `calificaciones.csv` y tambi√©n en `alumnos.csv`).
5. Apunta tus hallazgos en un documento de texto. Este ser√° nuestro punto de partida para la limpieza de datos en m√≥dulos posteriores.
