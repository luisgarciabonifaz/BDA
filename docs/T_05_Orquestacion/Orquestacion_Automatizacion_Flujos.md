# Orquestación y Automatización de Flujos

## 1. ¿Qué es la Orquestación de Datos?

La orquestación de datos es la coordinación de múltiples tareas para crear un proceso de flujo de datos completo y automatizado. Imagina que tienes una orquesta: el director es el orquestador, asegurándose de que cada músico (cada tarea) toque en el momento correcto para crear una melodía (el flujo de datos completo).



### 1.1. Diferencia entre Orquestación y Coreografía

- **Orquestación:** Es un enfoque **centralizado**. Hay un sistema (el orquestador) que tiene una visión global del proceso. Es como un director de orquesta que le dice a cada músico cuándo tocar. La orquestación es ideal para flujos de datos donde las dependencias son clave (ej. no puedes cargar datos si no has terminado de transformarlos).
- **Coreografía:** Es un enfoque **descentralizado**. Cada componente del sistema reacciona a los eventos de otros componentes, sin un controlador central. Es como en una fiesta donde la gente baila de forma libre, reaccionando a la música y a los demás sin un coreógrafo que les diga qué hacer. Es útil en sistemas distribuidos y asíncronos.

En la ingeniería de datos, la **orquestación** es el método preferido para los pipelines de ETL, ya que permite controlar el orden de ejecución, gestionar fallos y reintentos, y monitorear el progreso del flujo de datos de principio a fin.


## 2. Herramientas de Orquestación Basadas en Código

A diferencia de las herramientas visuales como NiFi, las herramientas de orquestación como Apache Airflow se basan en **código** para definir los flujos de trabajo. Esto las hace extremadamente flexibles, versionables y reutilizables.

### 2.1. Apache Airflow: El estándar de la industria

Airflow es una plataforma de código abierto para crear, programar y monitorizar flujos de trabajo de manera programática. Sus conceptos clave son:

- **DAG (Directed Acyclic Graph):** Es el corazón de Airflow. Un DAG es un **grafo acíclico dirigido** que representa el flujo de trabajo. "Dirigido" significa que las tareas tienen un orden específico (A → B → C), y "acíclico" significa que no hay bucles (una tarea no puede volver a una tarea anterior). Cada nodo del grafo es una tarea, y las flechas indican las dependencias.
- **Operadores (Operators):** Son las piezas de código que definen lo que una tarea debe hacer. Cada operador representa una tarea atómica. Ejemplos comunes incluyen:
  - `BashOperator`: Ejecuta un comando de shell.
  - `PythonOperator`: Ejecuta una función de Python.
  - `SimpleHttpOperator`: Envía una petición HTTP.
- **Tareas (Tasks):** Una instancia de un `Operador` dentro de un `DAG`.



### 2.2. Alternativas a Airflow

- **Mage:** Una herramienta más moderna y amigable para el desarrollo. Su enfoque se centra en el desarrollo interactivo y la sencillez, ideal para equipos pequeños o flujos de trabajo más sencillos.
- **Prefect:** Una plataforma que se presenta como una alternativa a Airflow, enfocada en la robustez y la facilidad de uso. Prefect se destaca por su manejo de los fallos y sus capacidades de reintentos inteligentes.


## 3. Integración de NiFi con Orquestadores

Una de las principales ventajas de herramientas como NiFi es su **API REST**. Esta interfaz nos permite interactuar con NiFi de manera programática, lo que significa que un orquestador como Airflow puede enviar comandos a NiFi.

El patrón de integración típico es:

1. **Airflow define el flujo de trabajo (DAG):** El DAG de Airflow contendrá tareas para los diferentes pasos de nuestro pipeline.
2. **Una tarea de Airflow envía una petición a la API de NiFi:** Esta petición puede, por ejemplo, iniciar o detener un grupo de procesos en NiFi.
3. **NiFi ejecuta el flujo de datos:** NiFi se encarga del procesamiento, la transformación y la carga de los datos.
4. **Airflow monitoriza el estado:** Airflow puede seguir el estado de la tarea de NiFi, sabiendo si se ejecutó con éxito o si falló.

Este enfoque separa la **orquestación** (quién, cuándo y en qué orden se ejecutan las tareas) de la **ejecución** (cómo se procesan realmente los datos), permitiendo que cada herramienta se enfoque en lo que mejor sabe hacer.


## 4. Profundizando en Apache Airflow

**Apache Airflow** es una plataforma de código abierto que se ha convertido en el estándar de la industria para orquestar flujos de trabajo de datos. A diferencia de NiFi, que se centra en el flujo y la transformación visual, Airflow se enfoca en la **programación y automatización** de tareas a gran escala.

### 4.1. Características Clave de Airflow

- **Basado en Código:** Los flujos de trabajo se definen como código Python, lo que permite la versionabilidad, las pruebas unitarias y la colaboración en equipo. Esto es una ventaja significativa sobre las herramientas con interfaces gráficas.
- **Flexibilidad:** Con su extensa biblioteca de **operadores**, Airflow puede interactuar con casi cualquier sistema, desde bases de datos hasta APIs y servicios en la nube (AWS, Google Cloud, Azure).
- **Interfaz de Usuario (UI):** Airflow incluye una interfaz web intuitiva para monitorizar el estado de los DAGs, ver los logs de las tareas, depurar fallos y gestionar las ejecuciones.
- **Escalabilidad:** Está diseñado para ejecutar miles de tareas de manera simultánea en un clúster de máquinas, lo que lo hace ideal para entornos de Big Data.

### 4.2. Componentes Principales**

Para entender cómo funciona Airflow, es crucial conocer sus componentes principales:

- **Scheduler (Planificador):** Es el "cerebro" de Airflow. Se encarga de programar los DAGs, gestionar las ejecuciones (DAG Runs) y disparar las tareas en el momento adecuado.
- **Webserver (Servidor Web):** El componente que ejecuta la interfaz de usuario de Airflow. Los usuarios interactúan con sus DAGs a través de esta interfaz.
- **Executor (Ejecutor):** Es el motor que realmente ejecuta las tareas. Airflow ofrece varios tipos de ejecutores (Sequential, Local, Celery, Kubernetes), que determinan cómo y dónde se ejecutan las tareas.
- **Worker (Trabajador):** En un clúster de Airflow, los workers son los procesos que ejecutan las tareas que el `Executor` les asigna.
- **Base de Datos de Metadatos:** Almacena el estado de los DAGs, la información de las tareas, los logs de ejecución y las configuraciones.



### 4.3. ¿Cómo funciona?

1. **Definición del DAG:** Un desarrollador escribe un archivo de Python que define un DAG. En este archivo, se especifican las tareas y el orden en el que deben ejecutarse (las dependencias).
2. **Detección y Programación:** El `Scheduler` busca nuevos archivos DAG en una carpeta específica. Una vez detectado, el `Scheduler` lo programa para su ejecución en base a la frecuencia que se haya definido (ej. una vez al día).
3. **Ejecución de Tareas:** Cuando llega el momento de la ejecución, el `Scheduler` le dice al `Executor` qué tareas deben ejecutarse. El `Executor` asigna estas tareas a los `Workers`.
4. **Monitoreo y Almacenamiento:** El `Webserver` muestra el estado de las tareas y los DAGs en tiempo real. Todos los eventos y logs se guardan en la base de datos de metadatos para su consulta y depuración.


### 4.4. Ejemplo Sencillo de un DAG

Aquí tienes un ejemplo de un DAG simple escrito en Python que simula un flujo de trabajo que primero descarga un archivo y luego lo procesa.

Python

```
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime, timedelta

# Define los argumentos por defecto para el DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Define el DAG
dag = DAG(
    'flujo_descarga_procesamiento_simple',
    default_args=default_args,
    description='Un DAG simple para descargar y procesar un archivo',
    schedule_interval=timedelta(days=1),
)

# Tarea 1: Descargar un archivo (usando un operador de bash para simular)
descargar_archivo = BashOperator(
    task_id='descargar_archivo',
    bash_command='echo "Simulando la descarga de un archivo..."',
    dag=dag,
)

# Tarea 2: Procesar el archivo descargado
procesar_archivo = BashOperator(
    task_id='procesar_archivo',
    bash_command='echo "Procesando el archivo descargado..."',
    dag=dag,
)

# Establecer la dependencia entre las tareas
descargar_archivo >> procesar_archivo
```

En este ejemplo, la línea `descargar_archivo >> procesar_archivo` define la dependencia: la tarea de `procesar_archivo` no se ejecutará hasta que la tarea de `descargar_archivo` haya finalizado con éxito. Este es el principio básico detrás de la orquestación con Airflow.

## 5. Práctica: Automatización con Airflow y NiFi

Esta práctica final unirá todos los conceptos del curso.

### 5.1. (P1 y P2): Creación de un DAG en Airflow para el Proyecto

1. **Configuración del Entorno:** Montaremos una instancia de Apache Airflow (típicamente con Docker Compose) y la configuraremos para que pueda comunicarse con nuestra instancia de NiFi.
2. **Creación del DAG:**
   - Escribiremos un archivo de Python para definir el DAG.
   - El DAG tendrá una única tarea principal.
   - Utilizaremos un `SimpleHttpOperator` para enviar una petición a la API de NiFi. La petición le dirá a NiFi que inicie la ejecución de nuestro grupo de procesos (`Process Group`) para los proyectos 1 y 2.
   - Programaremos el DAG para que se ejecute diariamente a una hora específica.
3. **Verificación:** Una vez en marcha, monitorizaremos el DAG desde la interfaz de Airflow para asegurarnos de que se ejecute en el horario programado. También revisaremos los registros de NiFi para confirmar que el flujo de datos se ha iniciado y completado correctamente.

Al final de esta práctica, habrás creado un sistema de procesamiento de datos **totalmente automatizado** y robusto, listo para entornos de producción.