
# Almacenamiento Moderno: Data Lake & Data Warehouse

Si en el m√≥dulo anterior aprendimos a "refinar" nuestros datos con Python y Pandas, ahora vamos a aprender a construir el "almac√©n" donde guardaremos ese valioso producto final. La forma en que almacenamos los datos es una de las decisiones de arquitectura m√°s cr√≠ticas que tomaremos. Una buena estrategia de almacenamiento garantiza que nuestras consultas sean r√°pidas, que nuestros sistemas sean escalables y que nuestros costes se mantengan bajo control.

En este m√≥dulo, exploraremos los tres grandes paradigmas de almacenamiento: el tradicional **Data Warehouse**, el flexible **Data Lake** y la moderna arquitectura h√≠brida, el **Data Lakehouse**. Tambi√©n descubriremos por qu√© el formato de fichero que elegimos (como **Parquet**) puede tener un impacto dr√°stico en el rendimiento. Finalmente, pondremos toda esta teor√≠a en pr√°ctica dise√±ando el coraz√≥n de nuestro proyecto de an√°lisis acad√©mico: un Data Warehouse con esquema en estrella.

## 1. Paradigmas de Almacenamiento: 

**¬øBiblioteca, Lago o Ambos?** La elecci√≥n de la arquitectura de almacenamiento depende del tipo de datos que tenemos y, sobre todo, de lo que queremos hacer con ellos.

### 1.1. El Data Warehouse (DWH): La Biblioteca Especializada

Imagina una gran biblioteca universitaria. Los libros (datos) no se aceptan de cualquier manera. Pasan por un riguroso proceso de selecci√≥n, catalogaci√≥n y clasificaci√≥n antes de ser colocados en estanter√≠as espec√≠ficas. Todo est√° perfectamente ordenado y estructurado para que un investigador (un analista de negocio) pueda encontrar la informaci√≥n que necesita para su tesis (un informe de BI) de la forma m√°s r√°pida y fiable posible.

- **Concepto Clave:** Es un repositorio central de datos **integrados, depurados y estructurados**, optimizado para el an√°lisis y la generaci√≥n de informes (Business Intelligence).
- **Schema-on-Write (Estructura al Escribir):** La caracter√≠stica que lo define. Antes de que un solo dato entre en el DWH, debemos haber definido una estructura r√≠gida (tablas, columnas, tipos de datos). Si los datos no cumplen con esa estructura, son rechazados o deben ser transformados.
- **Contenido:** Solo contiene la "joya de la corona": datos limpios, transformados y agregados que han sido validados. Es la **fuente √∫nica de la verdad (Single Source of Truth)** para la toma de decisiones estrat√©gicas.
- **Uso Principal:** Consultas SQL complejas, informes de BI (como los que haremos con Power BI) y cuadros de mando para la direcci√≥n.
- **Metodolog√≠as de Dise√±o:**
  - **Inmon (Top-down):** Propone construir un gran DWH corporativo centralizado y, a partir de √©l, crear peque√±os subconjuntos de datos llamados *Data Marts* para cada departamento.
  - **Kimball (Bottom-up):** Propone un enfoque m√°s √°gil. Se construyen primero los *Data Marts* enfocados en procesos de negocio espec√≠ficos (ventas, finanzas, etc.) y luego se unen para formar un DWH consolidado. **Para nuestro Proyecto 2, seguiremos la filosof√≠a de Kimball**, ya que dise√±aremos un Data Mart espec√≠fico para el an√°lisis de resultados acad√©micos.



### 1.2. El Data Lake: El Lago Natural

Ahora imagina un gran lago natural. Varios r√≠os (fuentes de datos) desembocan en √©l, arrastrando todo tipo de elementos: agua clara, barro, troncos, peces, plantas (datos estructurados, semi-estructurados y no estructurados). El lago lo acepta y lo almacena todo en su estado original, sin filtros.

- **Concepto Clave:** Es un repositorio de almacenamiento masivo que guarda enormes cantidades de **datos en su formato nativo y crudo**.
- **Schema-on-Read (Estructura al Leer):** La filosof√≠a opuesta al DWH. No se define ninguna estructura al guardar los datos. La responsabilidad de interpretar y dar forma a los datos recae en el analista o cient√≠fico de datos en el momento en que los va a utilizar.
- **Contenido:** ¬°De todo! Desde tablas de bases de datos y CSVs hasta logs de servidores, im√°genes, v√≠deos, clics de una web o datos de sensores de IoT. Nada se descarta.
- **Uso Principal:** Exploraci√≥n de datos, an√°lisis predictivo y Machine Learning, donde los cient√≠ficos de datos necesitan acceso a los datos originales y sin procesar para descubrir patrones ocultos.
- **El Gran Reto:** Si no se gestiona adecuadamente con metadatos y un cat√°logo, un Data Lake puede convertirse r√°pidamente en un **Data Swamp (Ci√©naga de Datos)**: un repositorio ca√≥tico y sin documentar donde es imposible encontrar nada de valor.

### 1.3. El Data Lakehouse: Lo Mejor de Ambos Mundos

¬øY si pudi√©ramos tener la escala y flexibilidad del lago, pero con la fiabilidad y el orden de la biblioteca? Esa es la promesa del Data Lakehouse. Es la arquitectura que est√° dominando el ecosistema de datos moderno.

- **Concepto Clave:** Es una nueva arquitectura que implementa funcionalidades de gesti√≥n y estructura propias de un Data Warehouse directamente sobre el almacenamiento de bajo coste de un Data Lake.
- **C√≥mo Funciona:** Se basa en una capa de metadatos y un formato de tabla de c√≥digo abierto (como Delta Lake, Apache Iceberg o Apache Hudi) que se coloca "encima" de los ficheros de datos (como Parquet) en el Data Lake. Esta capa a√±ade capacidades cruciales.
- **Superpoderes que aporta:**
  - **Transacciones ACID:** Garantiza que las operaciones sobre los datos sean fiables (o se completan todas, o no se completa ninguna), evitando la corrupci√≥n de datos.
  - **Time Travel (Viaje en el Tiempo):** Permite consultar el estado de una tabla en un punto exacto del pasado, lo que es incre√≠ble para la auditor√≠a y la depuraci√≥n.
  - **Rendimiento Optimizado:** Combina el poder del almacenamiento columnar con t√©cnicas de indexaci√≥n y caching para lograr velocidades de consulta similares a las de un DWH.
  - **Unifica el BI y el Machine Learning:** Los analistas de BI pueden ejecutar sus informes sobre datos fiables y actualizados, mientras que los cient√≠ficos de datos pueden trabajar con los mismos datos para entrenar sus modelos.


## 2. Formatos de Fichero Optimizados: El Secreto de la Velocidad

Guardar los datos en un CSV es f√°cil, pero terriblemente ineficiente para Big Data. La elecci√≥n del formato de fichero es clave.

### 2.1. Almacenamiento por Filas vs. por Columnas

Esta es la diferencia fundamental.

- **Basado en Filas (CSV, Avro):** Los datos de un mismo registro se guardan juntos en el disco. Es como leer un libro l√≠nea por l√≠nea.
  - **Ideal para:** Cargas transaccionales donde necesitas escribir o leer **un registro completo** r√°pidamente (ej. "dame toda la informaci√≥n del alumno con ID=50").
- **Basado en Columnas (Parquet, ORC):** Todos los datos de una misma columna se guardan juntos en el disco. Es como leer solo el primer cap√≠tulo de todos los libros de una estanter√≠a.
  - **Ideal para:** Cargas anal√≠ticas donde tus consultas solo implican **unas pocas columnas** de una tabla con muchas (ej. "calcula la media de las notas de todos los alumnos"). El sistema solo lee los datos de la columna "nota", ignorando el resto, lo que es incre√≠blemente r√°pido.



### 2.2. Apache Parquet: El Est√°ndar de Oro

**Parquet** es un formato de fichero de c√≥digo abierto, basado en columnas, que se ha convertido en el est√°ndar de facto en el ecosistema Big Data.

- **Rendimiento Superior:** Al ser columnar, las consultas anal√≠ticas son √≥rdenes de magnitud m√°s r√°pidas.
- **Alta Compresi√≥n:** Agrupar datos del mismo tipo permite aplicar algoritmos de compresi√≥n muy eficientes, reduciendo dr√°sticamente el espacio de almacenamiento (y por tanto, el coste).
- **Esquema Integrado:** El propio fichero Parquet almacena la informaci√≥n sobre la estructura de los datos (nombres de columnas, tipos de datos). Es auto-documentado.


## 3. Gobernanza y Calidad del Dato: Poniendo Orden

Tener mucha tecnolog√≠a no sirve de nada si los datos son un caos.

- **Data Governance (Gobernanza de Datos):** Es el conjunto de pol√≠ticas y procesos para gestionar los datos como un activo estrat√©gico. Responde a las preguntas: ¬øQui√©n es el due√±o de cada dato? ¬øQui√©n tiene permiso para verlo o modificarlo? ¬øCumple con las normativas de privacidad (GDPR)?
- **Data Quality (Calidad de Datos):** Se asegura de que los datos son aptos para su uso. Un dato de calidad debe ser: **Preciso, Completo, Consistente, V√°lido y Actualizado.**

## 3. Gobernanza y Calidad del Dato: Poniendo Orden

Tener una arquitectura de almacenamiento potente y formatos de fichero eficientes es solo la mitad de la batalla. Sin un conjunto de reglas, procesos y controles, nuestro brillante Data Lake puede convertirse r√°pidamente en una "ci√©naga de datos" (Data Swamp): un lugar ca√≥tico, poco fiable y, en √∫ltima- instancia, in√∫til. Esta secci√≥n trata sobre c√≥mo pasar de la anarqu√≠a de datos a una gesti√≥n estrat√©gica que genere confianza y valor.

### 3.1. Data Governance (Gobernanza de Datos): El Gobierno de Nuestros Datos

La gobernanza de datos es el marco general; el conjunto de pol√≠ticas, roles, est√°ndares y procesos que garantizan que los datos de una organizaci√≥n se gestionan como un **activo estrat√©gico**.

**Analog√≠a:** Piensa en el gobierno de una ciudad. El gobierno no construye cada casa, pero establece el plan urban√≠stico, los c√≥digos de construcci√≥n y las normativas (qui√©n puede construir, d√≥nde y c√≥mo) para asegurar que la ciudad crezca de forma ordenada, segura y funcional. De la misma manera, la gobernanza de datos establece las "leyes" para que los datos sean fiables y valiosos.

Sus pilares fundamentales son:

- **Propiedad y Responsabilidad (Ownership & Stewardship):** Asigna responsabilidades claras. ¬øQui√©n es el "due√±o" de los datos de los clientes? ¬øQui√©n es responsable de asegurar la calidad de los datos de ventas?
- **Pol√≠ticas y Est√°ndares:** Define reglas consistentes sobre c√≥mo nombrar, definir, almacenar y clasificar los datos (ej. "todos los ficheros de clientes deben contener una columna `id_cliente` de tipo texto").
- **Seguridad y Privacidad:** Controla qui√©n puede acceder a qu√© datos y bajo qu√© condiciones, asegurando el cumplimiento de normativas como el GDPR.


### 3.2. Data Quality Management (Gesti√≥n de la Calidad del Dato)

La gesti√≥n de la calidad es la implementaci√≥n pr√°ctica de las pol√≠ticas de gobernanza para asegurar que los datos son **aptos para su uso**. No es una tarea que se hace una sola vez, sino un proceso continuo de medici√≥n, monitorizaci√≥n y mejora.

Las dimensiones clave de la calidad de un dato son:

- **Precisi√≥n (Accuracy):** ¬øEl dato refleja correctamente el objeto o evento del mundo real? *Ej: ¬øLa direcci√≥n de env√≠o de un cliente en la base de datos es realmente donde vive?*
- **Completitud (Completeness):** ¬øTenemos todos los datos que necesitamos? ¬øFaltan valores en campos importantes? *Ej: Un registro de producto sin precio o sin stock.*
- **Consistencia (Consistency):** ¬øEl mismo dato es coherente a trav√©s de diferentes sistemas? *Ej: El cliente aparece como "CIFP Mislata" en el sistema de facturaci√≥n y como "Centro Integrado P√∫blico de Formaci√≥n Profesional Mislata" en el CRM.*
- **Validez (Validity):** ¬øEl dato se ajusta a los formatos y reglas definidos? *Ej: Un campo `codigo_postal` que contiene texto en lugar de 5 d√≠gitos num√©ricos.*
- **Puntualidad/Actualidad (Timeliness):** ¬øEl dato est√° disponible y actualizado cuando se necesita? *Ej: Un informe de ventas que se genera con datos de hace un mes tiene poca utilidad para tomar decisiones diarias.*

### 3.3. Componentes Clave para una Gobernanza Activa

Para que la gobernanza no sea solo un documento te√≥rico, se apoya en herramientas pr√°cticas que la hacen tangible y operativa.

##### **Cat√°logo de Datos (Data Catalog): El "Google" de tus Datos üó∫Ô∏è**


Un cat√°logo de datos es una herramienta centralizada que crea un inventario de todos los activos de datos de una organizaci√≥n. No almacena los datos en s√≠, sino los **metadatos** (los datos sobre los datos).

- **Analog√≠a:** No es la biblioteca (el Data Lake), sino el **fichero de fichas o el buscador online de la biblioteca**. Te permite buscar un libro (un dataset), te dice en qu√© estanter√≠a est√° (su ubicaci√≥n), de qu√© trata (descripci√≥n de columnas), qui√©n es el autor (el due√±o del dato) y si otros lectores lo han valorado bien (su nivel de calidad o popularidad).
- **¬øQu√© contiene?**
  - Descripciones de tablas y columnas en lenguaje de negocio.
  - Due√±os y responsables de los datos.
  - Puntuaciones de calidad.
  - Clasificaci√≥n de datos (ej. "P√∫blico", "Confidencial", "Sensible").
- **Su prop√≥sito:** Fomentar el **autoservicio**. Permite que tanto analistas como usuarios de negocio puedan **descubrir, entender y confiar** en los datos disponibles sin tener que preguntar constantemente al equipo de ingenier√≠a.

##### **Linaje de Datos (Data Lineage): El "GPS" del Dato üß¨**


El linaje de datos es la capacidad de visualizar el **ciclo de vida completo de los datos**, trazando su recorrido desde el origen hasta su destino final.

- **Analog√≠a:** Es como el **historial de seguimiento de un paquete de Amazon**. Te muestra d√≥nde se origin√≥, por qu√© almacenes ha pasado, qu√© transportista lo ha movido y d√≥nde se ha entregado.

- **Responde a preguntas cr√≠ticas:**

  - **Origen:** ¬øDe qu√© sistema o fichero proviene este n√∫mero en mi informe?
  - **Transformaciones:** ¬øQu√© c√°lculos o limpiezas se le han aplicado a este dato desde que se origin√≥?
  - **An√°lisis de Impacto:** Si voy a cambiar o eliminar esta tabla, ¬øqu√© informes, dashboards o modelos de Machine Learning dejar√°n de funcionar?
  - **Depuraci√≥n de Errores:** Si un dashboard muestra un dato incorrecto, el linaje nos permite "viajar hacia atr√°s" para encontrar el punto exacto donde se introdujo el error.

- **Importancia:** Es fundamental para la **confianza y la auditor√≠a**. Herramientas como **Apache NiFi**, que usaremos en el curso, tienen capacidades de linaje visual muy potentes, permitiendo ver en tiempo real la traza completa de cada dato que fluye por el sistema.

  

## 4. Pr√°ctica: Dise√±ando y Construyendo Nuestro Almac√©n

### 4.1. Pr√°ctica 1 (Proyecto 2): Dise√±o del Data Warehouse Acad√©mico

Vamos a aplicar la metodolog√≠a de Kimball para dise√±ar un *Data Mart* para nuestro proyecto. Usaremos un **Esquema en Estrella**, el modelo m√°s com√∫n y eficiente para BI.

1. **Tabla de Hechos (Fact Table):** Es la tabla central. Contiene las m√©tricas num√©ricas que queremos analizar. En nuestro caso, la **calificaci√≥n**. Tambi√©n contiene claves for√°neas que la unen a las dimensiones.
2. **Tablas de Dimensiones (Dimension Tables):** Son las tablas que rodean a la de hechos. Contienen el contexto descriptivo. Responden a las preguntas "qui√©n, qu√©, d√≥nde, cu√°ndo, por qu√©".

**Nuestro Dise√±o:**

- **`Fact_Calificaciones`** (Contiene las m√©tricas)
  - `id_calificacion` (Clave Primaria)
  - `calificacion_numerica` (La m√©trica principal)
  - `id_alumno_fk` (Clave for√°nea a Dim_Alumnos)
  - `id_modulo_fk` (Clave for√°nea a Dim_Modulos)
  - `id_tiempo_fk` (Clave for√°nea a Dim_Tiempo)
- **`Dim_Alumnos`** (Describe al estudiante)
  - `id_alumno_pk` (Clave Primaria)
  - `nombre_completo`
  - `turno` (Diurno, Semipresencial)
  - `grupo`
- **`Dim_Modulos`** (Describe la asignatura)
  - `id_modulo_pk` (Clave Primaria)
  - `nombre_modulo`
  - `ciclo_formativo` (DAM, DAW, etc.)
  - `grado` (Medio, Superior)
  - `familia_profesional` (Inform√°tica, Sanidad, etc.)
- **`Dim_Tiempo`** (Describe cu√°ndo ocurri√≥ la calificaci√≥n)
  - `id_tiempo_pk` (Clave Primaria)
  - `fecha_completa` (ej. 2025-03-15)
  - `a√±o` (2025)
  - `trimestre` (Q1)
  - `mes` (Marzo)
  - `dia_semana` (S√°bado)


### 4.2. Pr√°ctica 2 (Proyectos 1 y 2): De CSV a Parquet con Python y Pandas

Ahora, vamos a convertir los ficheros CSV limpios del m√≥dulo anterior al formato Parquet, mucho m√°s eficiente.

Python

```
# Fichero: convertir_a_parquet.py

import pandas as pd

# --- PROYECTO 2: DATOS ACAD√âMICOS ---
print("Procesando datos acad√©micos...")
# Cargar el fichero CSV limpio que generamos en el M√≥dulo 2
try:
    df_academico = pd.read_csv('datos_procesados/Calificaciones_limpio.csv', sep=';')

    # Guardar en formato Parquet
    # 'engine='pyarrow'' especifica la librer√≠a a usar (la m√°s com√∫n)
    # 'compression='snappy'' es un algoritmo de compresi√≥n r√°pido y eficiente
    output_path_academico = 'datos_almacenados/calificaciones.parquet'
    df_academico.to_parquet(output_path_academico, engine='pyarrow', compression='snappy')
    
    print(f"Datos acad√©micos guardados en formato Parquet en: {output_path_academico}")

except FileNotFoundError:
    print("No se encontr√≥ el fichero 'Calificaciones_limpio.csv'. Aseg√∫rate de haber completado la pr√°ctica del M√≥dulo 2.")


# --- PROYECTO 1: DATOS DE SENSORES ---
print("\nProcesando datos de sensores...")
# Cargar el fichero JSON que generamos
try:
    df_sensores = pd.read_json('datos_generados/sensor_data.json')

    # Guardar en formato Parquet
    output_path_sensores = 'datos_almacenados/sensores.parquet'
    df_sensores.to_parquet(output_path_sensores, engine='pyarrow', compression='snappy')

    print(f"Datos de sensores guardados en formato Parquet en: {output_path_sensores}")

except FileNotFoundError:
    print("No se encontr√≥ el fichero 'sensor_data.json'. Aseg√∫rate de haber completado la pr√°ctica del M√≥dulo 2.")
```

Este script coge nuestros datos procesados y los almacena en un formato columnar y comprimido, dej√°ndolos listos para ser utilizados por herramientas de procesamiento a gran escala como Apache NiFi en los pr√≥ximos m√≥dulos.