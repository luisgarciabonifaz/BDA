# BDA

## Temas


| M贸dulo                                                          | Horas|RA's|
| --------------------------------------------------------------- | ---- |----|
| M贸dulo 0: Infraestructura Big Data                              | 2    ||
| M贸dulo 1: Fundamentos y Ecosistema Moderno de Big Data          | 9    |RA1|
| M贸dulo 2: Bases de Datos NoSQL en la Pr谩ctica                   | 11   |RA2,RA3|
| M贸dulo 3: Almacenamiento Moderno: Data Lake & Data Warehouse    | 13   |RA2.RA3|
| M贸dulo 4: Procesamiento y Flujos de Datos (ETL) con Apache NiFi | 18   |+RA3|
| M贸dulo 5: Orquestaci贸n de Flujos                                | 20   |RA4|
| M贸dulo 6: Inteligencia de Negocio y Visualizaci贸n con Power BI  | 16   |RA5+RA2|
| M贸dulo 7: Visualizaci贸n de Datos en Tiempo Real y Cierre        | 11   |RA6+RA4|
| **Total**                                                       |**95**|

Exportar a Hojas de c谩lculo

RA3 de BDA Se elimina

**RA's SBD**
-----------
RA1 --> Spark
RA2 --> Modulo 6
RA3 --> Modulo 4
RA4 --> Modulo 7


## Descripci贸n



## M贸dulo 1: Fundamentos y Ecosistema Moderno de Big Data (8 horas)
Se introduce el campo del Big Data, no solo con la teor铆a cl谩sica, sino presentando desde el primer d铆a el "Modern Data Stack" y los dos proyectos que se desarrollar谩n a lo largo del curso.

1.1. 驴Qu茅 es Big Data?

Las 5 'V' (Volumen, Velocidad, Variedad, Veracidad, Valor).

El rol del Ingeniero de Datos, Analista de Datos y Cient铆fico de Datos.

1.2. El Ecosistema de Datos Moderno.

Visi贸n general de arquitecturas Cloud (AWS, Azure, GCP).

Componentes clave: Ingesta, Almacenamiento (Data Lake, Data Warehouse), Procesamiento, Orquestaci贸n y Visualizaci贸n.

Introducci贸n a conceptos como Data Lakehouse e ELT.

1.3. Presentaci贸n de los Proyectos del Curso. 

Proyecto 1 (P1): Smart City. Se presenta como un reto de datos en streaming (IoT).

Proyecto 2 (P2): An谩lisis Acad茅mico. Se presenta como un reto de Business Intelligence y Data Warehousing.

1.4. Pr谩ctica Inicial:

(P1): Dise帽o inicial de las 3 entidades (Sensor Temperatura, CO2, Calidad Agua) en un documento.

(P2): An谩lisis exploratorio inicial de los ficheros CSV proporcionados.

## M贸dulo 2: Bases de Datos NoSQL en la Pr谩ctica con FIWARE (10 horas)
Introducci贸n al m贸dulo:
No todos los datos encajan bien en las tablas y filas de las bases de datos tradicionales. Este m贸dulo explora el mundo de las bases de datos NoSQL, dise帽adas para la flexibilidad y la escala. Nos centraremos en un caso de uso muy pr谩ctico: la plataforma FIWARE para IoT, que utiliza internamente bases de datos NoSQL para gestionar los datos de los sensores en tiempo real de nuestro Proyecto 1.

4.1. Introducci贸n a NoSQL: Cu谩ndo y por qu茅 usarlas frente a SQL.

4.2. Tipos de Bases de Datos NoSQL.

Documentales (MongoDB), Clave-Valor (Redis, DynamoDB), Columnares (Cassandra) y de Grafos (Neo4j).

4.3. FIWARE y su integraci贸n con Bases de Datos NoSQL.

El rol del Orion Context Broker como gestor de estado.

C贸mo FIWARE se integra con bases de datos como MongoDB (para el estado actual) y CrateDB o TimescaleDB (para el hist贸rico de datos).

4.4. Pr谩ctica (P1):

Configuraci贸n del entorno FIWARE (Orion Context Broker y una base de datos para persistencia).

Creaci贸n de las 3 entidades definidas en el M贸dulo 1 a trav茅s de la API de Orion.

Creaci贸n de una suscripci贸n para que cualquier cambio en las entidades se guarde autom谩ticamente en la base de datos hist贸rica.

Cargar los 400 datos por atributo usando el script de Python del M贸dulo 2 para enviar actualizaciones a Orion.

## M贸dulo 3: Almacenamiento Moderno: Data Lake & Data Warehouse (12 horas)
Introducci贸n al m贸dulo:
Un proyecto de datos es tan s贸lido como sus cimientos, y esos cimientos son el almacenamiento. En este m贸dulo, profundizaremos en los dos pilares del almacenamiento de Big Data: el Data Lake, para guardar datos en su formato original, y el Data Warehouse, para almacenar datos estructurados y listos para el an谩lisis. Veremos c贸mo ambos conceptos pueden convivir e incluso fusionarse en la arquitectura moderna del Data Lakehouse.

3.1. Paradigmas de Almacenamiento.

Data Warehouse (DWH): Enfocado en el an谩lisis de negocio. Estructura y prop贸sito (modelado dimensional con esquemas en estrella/copo de nieve).

Data Lake (DL): Flexibilidad para datos crudos, semi-estructurados y no estructurados.

El Data Lakehouse: La arquitectura h铆brida que combina la flexibilidad del Data Lake con las capacidades de gesti贸n del Data Warehouse (ej. Databricks Delta Lake, Apache Iceberg).

3.2. Formatos de Fichero Optimizados para Big Data.

Parquet: El est谩ndar de facto. Almacenamiento columnar, compresi贸n y optimizaci贸n de consultas.

Otros formatos como ORC y Avro.

3.3. Gobernanza y Calidad del Dato (Data Governance).

Conceptos de linaje de datos, catalogaci贸n y gesti贸n de la calidad.

3.4. Pr谩ctica:

(P2): Dise帽ar en detalle el esquema en estrella para el Data Warehouse acad茅mico. Definir las tablas de hechos (calificaciones) y las tablas de dimensiones (alumnos, ciclos, tiempo, etc.) con sus respectivas columnas y tipos de datos.

(P1 y P2): Usar el script de Python del m贸dulo anterior para convertir los datos generados (sensores) y los datos limpios (acad茅micos) a formato Parquet.


## M贸dulo 4: Procesamiento y Flujos de Datos (ETL) con Apache NiFi (16 horas)
Introducci贸n al m贸dulo:
Este es el coraz贸n pr谩ctico del curso. Aqu铆 aprenderemos a mover, transformar y dirigir los datos de un lugar a otro usando Apache NiFi. NiFi es una herramienta visual e interactiva que permite dise帽ar flujos de datos (ETL) complejos de forma intuitiva, conectando diferentes sistemas y procesando la informaci贸n en tiempo real o por lotes. Es la herramienta perfecta para construir los pipelines de nuestros dos proyectos.

5.1. Introducci贸n a Apache NiFi.

Arquitectura y conceptos clave: FlowFile, Procesador, Conexi贸n y Process Group.

La interfaz gr谩fica de NiFi: Construyendo flujos de datos arrastrando y soltando componentes.

5.2. Procesadores Esenciales de NiFi.

Ingesta de datos: GetFile, ListenHTTP, GetSFTP.

Transformaci贸n: JoltTransformJSON, ReplaceText, UpdateAttribute.

Enrutamiento: RouteOnAttribute, RouteOnContent.

Carga de datos: PutFile, PutDatabaseRecord.

5.3. Data Provenance: Trazabilidad total del dato.

C贸mo NiFi registra cada paso que sigue un dato, garantizando el linaje y facilitando la depuraci贸n.

5.4. Pr谩ctica:

(P1): Crear un flujo en NiFi que:

Escuche las notificaciones de FIWARE (v铆a HTTP).

Extraiga los datos de los sensores del cuerpo de la notificaci贸n.

Calcule los agregados diarios (m谩ximo, m铆nimo, media).

Guarde estos agregados en el Data Lake en formato Parquet.

(P2): Crear un flujo en NiFi que:

Lea los ficheros CSV limpios del sistema de archivos.

Realice las uniones (Join) y transformaciones necesarias para construir las tablas de hechos y dimensiones.

Cargue los datos procesados en las tablas del Data Warehouse.

## M贸dulo 5: Orquestaci贸n y Automatizaci贸n de Flujos (10 horas)
Introducci贸n al m贸dulo:
Un flujo de datos no es 煤til si tenemos que ejecutarlo manualmente cada vez. La orquestaci贸n consiste en programar, automatizar y gestionar la ejecuci贸n de nuestros pipelines de datos. Aunque NiFi tiene sus propias capacidades de programaci贸n, lo integraremos con herramientas est谩ndar de la industria como Apache Airflow para gestionar dependencias complejas y tener un control centralizado de todos nuestros procesos.

6.1. 驴Qu茅 es la Orquestaci贸n de Datos?

Diferencia entre orquestaci贸n y coreograf铆a.

6.2. Herramientas de Orquestaci贸n Basadas en C贸digo.

Apache Airflow: El est谩ndar de la industria. Conceptos de DAG, Operadores y Tareas.

Alternativas como Mage o Prefect.

6.3. Integraci贸n de NiFi con Orquestadores.

C贸mo usar la API de NiFi para iniciar y detener flujos de datos desde una herramienta externa como Airflow.

6.4. Pr谩ctica:

(P1 y P2): Crear un DAG (Directed Acyclic Graph) simple en Airflow que se ejecute diariamente y que, a trav茅s de la API de NiFi, inicie el flujo de datos correspondiente a cada proyecto. Esto simula un entorno de producci贸n real donde los procesos ETL est谩n totalmente automatizados.

## M贸dulo 6: Inteligencia de Negocio y Visualizaci贸n con Power BI (14 horas)
Introducci贸n al m贸dulo:
Los datos solo tienen valor si podemos convertirlos en conocimiento. Este m贸dulo se centra en la 煤ltima milla del viaje del dato: la visualizaci贸n y la inteligencia de negocio (BI). Aprenderemos a usar Power BI, una de las herramientas l铆deres del mercado, para conectar nuestros datos procesados, modelarlos y crear informes interactivos que respondan a preguntas de negocio clave para nuestro Proyecto 2.

7.1. Fundamentos de Business Intelligence (BI).

7.2. Modelado de Datos para BI con Power BI.

Conexi贸n a fuentes de datos (ficheros Parquet, bases de datos).

Importancia del esquema en estrella creado previamente.

Creaci贸n de relaciones, jerarqu铆as y columnas calculadas.

Introducci贸n a DAX (Data Analysis Expressions) para crear m茅tricas personalizadas.

7.3. Principios de Dise帽o de Dashboards Efectivos.

7.4. Pr谩ctica (P2):

Conectar Power BI a las tablas del Data Warehouse generadas por NiFi.

Implementar el modelo de datos (relaciones entre tablas de hechos y dimensiones) en Power BI.

Crear el cuadro de mando de resultados acad茅micos, incluyendo los filtros solicitados (Familia, Curso, Grado, etc.) y al menos 2 visualizaciones interactivas y 2 m茅tricas calculadas con DAX.

## M贸dulo 7: Visualizaci贸n de Datos en Tiempo Real y Cierre (10 horas)
Introducci贸n al m贸dulo:
Para datos que cambian constantemente, como los de nuestro proyecto de Smart City, necesitamos herramientas de visualizaci贸n diferentes. En este m贸dulo final, aprenderemos a usar Grafana, el est谩ndar para la monitorizaci贸n y visualizaci贸n de datos de series temporales. Cerraremos el curso preparando la documentaci贸n final y presentando los dos proyectos completos, demostrando as铆 el dominio de todo el ciclo de vida del dato.

8.1. Herramientas para Dashboards en Tiempo Real.

Grafana: El est谩ndar para m茅tricas y datos de series temporales (Time Series).

8.2. Conceptos Clave en Grafana.

Data Sources, Queries, Panels, Variables y Alertas.

8.3. Pr谩ctica (P1):

Conectar Grafana a la base de datos donde FIWARE almacena el hist贸rico de los sensores.

Crear el panel de control con 4 visualizaciones (ej. un gr谩fico de temperatura a lo largo del tiempo, un medidor para el nivel de CO2 actual, etc.).

Implementar una variable para poder seleccionar el sensor a visualizar.

Configurar una alerta que se dispare si un valor (ej. temperatura) supera un umbral definido.

8.4. Documentaci贸n y Presentaci贸n de Proyectos.

Elaboraci贸n del documento explicativo final para ambos proyectos, detallando la arquitectura, los flujos de datos y las decisiones de dise帽o.

Presentaci贸n final de resultados.

